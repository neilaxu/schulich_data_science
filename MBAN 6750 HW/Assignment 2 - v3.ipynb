{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "customers_df = pd.read_csv('https://raw.githubusercontent.com/delinai/schulich_ds1_2024/main/Datasets/customers_final.csv')\n",
    "engagements_df = pd.read_csv('https://raw.githubusercontent.com/delinai/schulich_ds1_2024/main/Datasets/engagements_final.csv')\n",
    "marketing_df = pd.read_csv('https://raw.githubusercontent.com/delinai/schulich_ds1_2024/main/Datasets/marketing_final.csv')\n",
    "transactions_df = pd.read_csv('https://raw.githubusercontent.com/delinai/schulich_ds1_2024/main/Datasets/transactions_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "customers_df['join_date'] = pd.to_datetime(customers_df['join_date'])\n",
    "customers_df['last_purchase_date'] = pd.to_datetime(customers_df['last_purchase_date'])\n",
    "marketing_df['campaign_date'] = pd.to_datetime(marketing_df['campaign_date'])\n",
    "transactions_df['transaction_date'] = pd.to_datetime(transactions_df['transaction_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff date\n",
    "cutoff_date = pd.to_datetime('2024-07-17')\n",
    "\n",
    "# Convert dates later than 7/17/2024 to 7/17/2024\n",
    "customers_df['join_date'] = customers_df['join_date'].apply(lambda x: x if x <= cutoff_date else cutoff_date)\n",
    "customers_df['last_purchase_date'] = customers_df['last_purchase_date'].apply(lambda x: x if x <= cutoff_date else cutoff_date)\n",
    "marketing_df['campaign_date'] = marketing_df['campaign_date'].apply(lambda x: x if x <= cutoff_date else cutoff_date)\n",
    "transactions_df['transaction_date'] = transactions_df['transaction_date'].apply(lambda x: x if x <= cutoff_date else cutoff_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate marketing responses\n",
    "marketing_agg = marketing_df.groupby('customer_id')['response'].value_counts().unstack(fill_value=0).reset_index()\n",
    "marketing_agg.columns = ['customer_id', 'no_response', 'yes_response']\n",
    "\n",
    "# Aggregate transaction data\n",
    "transactions_agg = transactions_df.groupby('customer_id').agg(\n",
    "    total_transaction_amount=pd.NamedAgg(column='transaction_amount', aggfunc='sum'),\n",
    "    number_of_transactions=pd.NamedAgg(column='transaction_id', aggfunc='count')\n",
    ").reset_index()\n",
    "\n",
    "# Merge all dataframes\n",
    "merged_df = customers_df.merge(engagements_df, on='customer_id', how='left') \\\n",
    "                        .merge(marketing_agg, on='customer_id', how='left') \\\n",
    "                        .merge(transactions_agg, on='customer_id', how='left')\n",
    "\n",
    "# Fill NaN values for no_response and yes_response with 0\n",
    "merged_df['no_response'] = merged_df['no_response'].fillna(0)\n",
    "merged_df['yes_response'] = merged_df['yes_response'].fillna(0)\n",
    "\n",
    "# Fill NaN values for total_transaction_amount and number_of_transactions with 0\n",
    "merged_df['total_transaction_amount'] = merged_df['total_transaction_amount'].fillna(0)\n",
    "merged_df['number_of_transactions'] = merged_df['number_of_transactions'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute 'Unknown' for missing gender values\n",
    "merged_df['gender'] = merged_df['gender'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing age with KNN\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert date columns to numeric\n",
    "merged_df['join_date_numeric'] = merged_df['join_date'].astype('int64') // 10**9\n",
    "merged_df['last_purchase_date_numeric'] = merged_df['last_purchase_date'].astype('int64') // 10**9\n",
    "\n",
    "# Encode categorical variables\n",
    "le_gender = LabelEncoder()\n",
    "merged_df['gender_encoded'] = le_gender.fit_transform(merged_df['gender'])\n",
    "\n",
    "le_location = LabelEncoder()\n",
    "merged_df['location_encoded'] = le_location.fit_transform(merged_df['location'])\n",
    "\n",
    "# Prepare data for KNN imputation\n",
    "impute_cols = ['age', 'join_date_numeric', 'last_purchase_date_numeric', 'gender_encoded', 'location_encoded']\n",
    "impute_data = merged_df[impute_cols]\n",
    "\n",
    "# Perform KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_data = knn_imputer.fit_transform(impute_data)\n",
    "\n",
    "# Assign imputed age values back to the dataframe\n",
    "merged_df['age'] = imputed_data[:, 0]\n",
    "\n",
    "# Drop intermediate columns\n",
    "merged_df = merged_df.drop(columns=['join_date_numeric', 'last_purchase_date_numeric', 'gender_encoded', 'location_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Recency, Frequency, and Monetary Value (RFM) features\n",
    "import numpy as np\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = pd.to_datetime('2024-07-17')\n",
    "\n",
    "# Calculate the joined month (days difference and convert to months)\n",
    "merged_df['joined_days'] = (cutoff_date - merged_df['join_date']).dt.days\n",
    "merged_df['joined_month'] = (merged_df['joined_days'] / 30.44).astype(int)\n",
    "merged_df['joined_month'] = merged_df['joined_month'].replace(0, np.nan)\n",
    "\n",
    "# Calculate recency in days and convert to months\n",
    "merged_df['recency_days'] = (cutoff_date - merged_df['last_purchase_date']).dt.days\n",
    "merged_df['recency'] = (merged_df['recency_days'] / 30.44).astype(int)\n",
    "\n",
    "# Calculate frequency (number of transactions per month)\n",
    "merged_df['frequency'] = merged_df['number_of_transactions'] / merged_df['joined_month']\n",
    "\n",
    "# Calculate monetary value (transaction value per month)\n",
    "merged_df['clv_monthly'] = merged_df['total_transaction_amount'] / merged_df['joined_month']\n",
    "\n",
    "# Fill NaN values for joined_month with a placeholder\n",
    "merged_df['joined_month'] = merged_df['joined_month'].fillna('.')\n",
    "\n",
    "# Drop intermediate columns\n",
    "merged_df = merged_df.drop(columns=['joined_days', 'recency_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of 'yes_response' per transaction\n",
    "merged_df['yes_response_per_transaction'] = merged_df['yes_response'] / merged_df['number_of_transactions']\n",
    "\n",
    "# Calculate the number of 'no_response' per transaction\n",
    "merged_df['no_response_per_transaction'] = merged_df['no_response'] / merged_df['number_of_transactions']\n",
    "\n",
    "# Calculate the number of site visits per transaction\n",
    "merged_df['site_visits_per_transaction'] = merged_df['number_of_site_visits'] / merged_df['number_of_transactions']\n",
    "\n",
    "# Calculate the number of emails opened per transaction\n",
    "merged_df['emails_opened_per_transaction'] = merged_df['number_of_emails_opened'] / merged_df['number_of_transactions']\n",
    "\n",
    "# Calculate the number of clicks per transaction\n",
    "merged_df['clicks_per_transaction'] = merged_df['number_of_clicks'] / merged_df['number_of_transactions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 80-20 rule, sorting the customer with clv_monthly\n",
    "# labelling the highest clv_monthly customers who cumulatively contribute 80% of the total monthly clv as 'High-Value'\n",
    "# other as ‘Low-Value’\n",
    "\n",
    "# Calculate the cumulative contribution of each customer to the total monthly CLV\n",
    "merged_df = merged_df.sort_values(by='clv_monthly', ascending=False)\n",
    "merged_df['cumulative_clv'] = merged_df['clv_monthly'].cumsum()\n",
    "total_clv = merged_df['clv_monthly'].sum()\n",
    "merged_df['cumulative_clv_percentage'] = merged_df['cumulative_clv'] / total_clv\n",
    "\n",
    "# Label customers based on their contribution to the total monthly CLV\n",
    "merged_df['value_label'] = np.where(merged_df['cumulative_clv_percentage'] <= 0.80, 'High-Value', 'Low-Value')\n",
    "\n",
    "# Drop intermediate columns\n",
    "merged_df = merged_df.drop(columns=['cumulative_clv', 'cumulative_clv_percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of merged_df as eda_df\n",
    "eda_df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_label\n",
      "Low-Value     0.6493\n",
      "High-Value    0.3507\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the imbalance\n",
    "# Calculate the distribution of the target variable\n",
    "class_distribution = eda_df['value_label'].value_counts(normalize=True)\n",
    "\n",
    "# Print the class distribution\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X variables and y variable\n",
    "# Define the y variable\n",
    "y = merged_df['value_label']\n",
    "\n",
    "# Define the X variables\n",
    "X = merged_df[['age', 'gender', 'recency', 'frequency', 'yes_response_per_transaction', \n",
    "               'no_response_per_transaction', 'site_visits_per_transaction', \n",
    "               'emails_opened_per_transaction', 'clicks_per_transaction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the features using MinMaxScaler\n",
    "# encode the categorical variables using OneHotEncoder\n",
    "# split the data into training and testing sets\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Apply MinMaxScaler to X\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X[['age', 'recency', 'frequency', 'yes_response_per_transaction',\n",
    "                                   'no_response_per_transaction', 'site_visits_per_transaction',\n",
    "                                   'emails_opened_per_transaction', 'clicks_per_transaction']])\n",
    "\n",
    "# Encode the gender column in X\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "gender_encoded = encoder.fit_transform(X[['gender']]).toarray()\n",
    "\n",
    "# Concatenate scaled numerical features and encoded categorical features\n",
    "X_encoded = np.hstack((X_scaled, gender_encoded))\n",
    "\n",
    "# Encode the y variable\n",
    "y_encoded = encoder.fit_transform(y.to_frame()).toarray()\n",
    "\n",
    "# Split the data into training and testing sets, reserving 30% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.30, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8630000000000001\n",
      "0.8752857142857143\n",
      "0.8022857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define numeric and categorical columns\n",
    "numeric_columns = ['age', 'recency', 'frequency', 'yes_response_per_transaction', \n",
    "                   'no_response_per_transaction', 'site_visits_per_transaction', \n",
    "                   'emails_opened_per_transaction', 'clicks_per_transaction']\n",
    "categorical_columns = ['gender']\n",
    "\n",
    "# Create a pre-processing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numeric_columns),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the models\n",
    "knn = KNeighborsClassifier()\n",
    "logreg = LogisticRegression()\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Split the data into training and testing sets, reserving 30% for testing\n",
    "X = merged_df[numeric_columns + categorical_columns]\n",
    "y = merged_df['value_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Create pipelines for each model\n",
    "knn_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', knn)])\n",
    "logreg_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', logreg)])\n",
    "nb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', nb)])\n",
    "\n",
    "# Evaluate the models using cross-validation\n",
    "knn_scores = cross_val_score(knn_pipeline, X_train, y_train, cv=5)\n",
    "logreg_scores = cross_val_score(logreg_pipeline, X_train, y_train, cv=5)\n",
    "nb_scores = cross_val_score(nb_pipeline, X_train, y_train, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(np.mean(knn_scores))\n",
    "print(np.mean(logreg_scores))\n",
    "print(np.mean(nb_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__metric': 'manhattan', 'classifier__n_neighbors': 11, 'classifier__weights': 'distance'}\n",
      "Best Score: 0.8902857142857142\n"
     ]
    }
   ],
   "source": [
    "# Improved KNN Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for KNN\n",
    "param_grid = {\n",
    "    'classifier__n_neighbors': [5, 7, 9, 11, 13, 15, 35, 45, 55],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Create the pipeline for KNN\n",
    "knn_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(knn_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Score: {best_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
